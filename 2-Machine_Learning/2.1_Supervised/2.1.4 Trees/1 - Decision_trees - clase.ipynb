{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQGm-heq1Tua"
   },
   "source": [
    "# **Chapter 6 – Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7_ZAC7M1Tub"
   },
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 6._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyO_LmBX1Tuc"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Qv5M-b_1Tud"
   },
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed, as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W9NzCNFg1Tud"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\\n    print(\"Saving figure\", fig_id)\\n    if tight_layout:\\n        plt.tight_layout()\\n    plt.savefig(path, format=fig_extension, dpi=resolution)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "'''\n",
    "\n",
    "# Where to save the figures\n",
    "\n",
    "'''\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9v2CRxV1Tuk"
   },
   "source": [
    "# Training and visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cmfxZUi1Tul",
    "outputId": "3e5a601c-ef12-4e71-e690-30d741bdbad3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TAFvUu3T1Tuv",
    "outputId": "e2ed9f46-8807-42e3-8c5a-ccc7a0251b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom matplotlib.colors import ListedColormap\\n\\ndef plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\\n    x1s = np.linspace(axes[0], axes[1], 100)\\n    x2s = np.linspace(axes[2], axes[3], 100)\\n    x1, x2 = np.meshgrid(x1s, x2s)\\n\\n\\n    \\n    X_new = np.c_[x1.ravel(), x2.ravel()] \\n    \\n\\n    \\n\\n    \\n    y_pred = clf.predict(X_new).reshape(x1.shape)\\n    \\n\\n    \\n\\n    \\n    # pintamos las zonas y fronteras\\n    \\n    custom_cmap = ListedColormap([\\'#fafab0\\',\\'#9898ff\\'])\\n    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\\n    if not iris:\\n        custom_cmap2 = ListedColormap([\\'#7d7d58\\',\\'#4c4c7f\\',\\'#507d50\\'])\\n        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\\n    if plot_training:\\n        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\\n        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"gs\", label=\"Iris versicolor\")\\n        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"b^\", label=\"Iris virginica\")\\n        plt.axis(axes)\\n    if iris:\\n        plt.xlabel(\"Petal length\", fontsize=14)\\n        plt.ylabel(\"Petal width\", fontsize=14)\\n    else:\\n        plt.xlabel(r\"$x_1$\", fontsize=18)\\n        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\\n    if legend:\\n        plt.legend(loc=\"lower right\", fontsize=14)\\n\\nplt.figure(figsize=(8, 4))\\nplot_decision_boundary(tree_clf, X, y)\\nplt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\\nplt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\\nplt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\\nplt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\\nplt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\\nplt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\\nplt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\\n\\nsave_fig(\"decision_tree_decision_boundaries_plot\")\\nplt.show();\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "\n",
    "    \n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()] \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # pintamos las zonas y fronteras\n",
    "    \n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"gs\", label=\"Iris versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"b^\", label=\"Iris virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
    "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
    "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
    "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
    "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
    "\n",
    "save_fig(\"decision_tree_decision_boundaries_plot\")\n",
    "plt.show();\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE4rKy9B1Tux"
   },
   "source": [
    "# Predicting classes and class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffYy1rsq1Tuy",
    "outputId": "a2167a52-0e90-44a1-cf13-5dc2da6ab436"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7gv1o9g1Tu1",
    "outputId": "0209ff8d-65c9-4991-997a-a7b16a95864f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXVXoNSr1Tu5"
   },
   "source": [
    "# Sensitivity to training set details\n",
    "The main issue with Decision Trees is that they are very sensitive to small variations in the training data. For example, if you just remove the widest Iris versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model represented before. As you can see, it looks very different from the previous Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dSMJx8D1Tu5",
    "outputId": "273e3246-ecbb-4297-f1db-7d0478201d25"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr7lIoKY1Tu9",
    "outputId": "35efa189-e25d-488c-ae1e-d4a0fddb96cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3ICJntW1TvC",
    "outputId": "2fe177c0-3f0b-41aa-fc1f-39a4f9879011"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left the Decision Tree is trained with the default hyperparameters (i.e., no restrictions), and on the right it’s trained with min_samples_leaf=4. It is quite obvious that the model on the left is overfitting, and the model on the right will probably generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFC3rMy61TvH",
    "outputId": "75b6d06d-c4d9-4395-bfd4-4f8f0a7d94fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LgP5P15K1TvM",
    "outputId": "e0a77ab3-7809-4182-9268-352b8a0ed458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nangle = np.pi / 180 * 20\\nrotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\\nXr = X.dot(rotation_matrix)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "angle = np.pi / 180 * 20\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xr = X.dot(rotation_matrix)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CZBuCMK01TvO",
    "outputId": "f5606e3f-df26-4cbf-8b45-db0cfafc6252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.random.seed(6)\\nXs = np.random.rand(100, 2) - 0.5\\nys = (Xs[:, 0] > 0).astype(np.float32) * 2\\n\\nangle = np.pi / 4\\nrotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\\nXsr = Xs.dot(rotation_matrix)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "np.random.seed(6)\n",
    "Xs = np.random.rand(100, 2) - 0.5\n",
    "ys = (Xs[:, 0] > 0).astype(np.float32) * 2\n",
    "\n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85i9ATWM1TvR"
   },
   "source": [
    "# Regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "PyPWjPFt1TvS"
   },
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "f7VetGJg1TvU",
    "outputId": "4641b81c-fd2b-4483-84e5-a7cbb265a2e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HHT2VVch1TvX",
    "outputId": "378c5ec2-823d-4ff0-bebd-17e07a9ebfe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\\n    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\\n    y_pred = tree_reg.predict(x1)\\n    plt.axis(axes)\\n    plt.xlabel(\"$x_1$\", fontsize=18)\\n    if ylabel:\\n        plt.ylabel(ylabel, fontsize=18, rotation=0)\\n    plt.plot(X, y, \"b.\")\\n    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\\\hat{y}$\")\\n\\nplt.figure(figsize=(11, 4))\\nplt.subplot(121)\\nplot_regression_predictions(tree_reg1, X, y)\\nfor split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\\n    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\\nplt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\\nplt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\\nplt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\\nplt.legend(loc=\"upper center\", fontsize=18)\\nplt.title(\"max_depth=2\", fontsize=14)\\n\\nplt.subplot(122)\\nplot_regression_predictions(tree_reg2, X, y, ylabel=None)\\nfor split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\\n    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\\nfor split in (0.0458, 0.1298, 0.2873, 0.9040):\\n    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\\nplt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\\nplt.title(\"max_depth=3\", fontsize=14)\\n\\nsave_fig(\"tree_regression_plot\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_plot\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8VtV9Jjw1Tvg",
    "outputId": "a0d42e39-8854-4d5a-c8a5-b07e341c151c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(11, 4))\\n\\nplt.subplot(121)\\nplt.plot(X, y, \"b.\")\\nplt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\\\hat{y}$\")\\nplt.axis([0, 1, -0.2, 1.1])\\nplt.xlabel(\"$x_1$\", fontsize=18)\\nplt.ylabel(\"$y$\", fontsize=18, rotation=0)\\nplt.legend(loc=\"upper center\", fontsize=18)\\nplt.title(\"No restrictions\", fontsize=14)\\n\\nplt.subplot(122)\\nplt.plot(X, y, \"b.\")\\nplt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\\\hat{y}$\")\\nplt.axis([0, 1, -0.2, 1.1])\\nplt.xlabel(\"$x_1$\", fontsize=18)\\nplt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\\n\\nsave_fig(\"tree_regression_regularization_plot\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_regularization_plot\")\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "decision_trees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {
   "height": "309px",
   "width": "468px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
